{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Zero order methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append path for local library, data, and image import\n",
    "import sys\n",
    "sys.path.append('./chapter_2_images') \n",
    "\n",
    "# image paths\n",
    "image_path_1 = 'chapter_2_images/bigpicture_regression_optimization.png'\n",
    "image_path_2 = 'chapter_2_images/bigpicture_classification_optimization.png'\n",
    "\n",
    "# standard imports\n",
    "import IPython\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1  Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of determining the smallest (or largest) value a function can take, referred to as its *global minimum* (or *global maximum*), is a centuries old pursuit that has numerous applications throughout the sciences and engineering.  In this Chapter we begin our investigation of mathematical optimization by describing the *zero order optimization* techniques.  While not always the most powerful optimization tools at our disposal, zero order techniques are conceptually the simplest tools available to us - requiring the least amount of intellectual machinery and jargon to describe.  Because of this discussing zero order methods first allows us to lay bear, in a simple setting, a range of crucial concepts we will see throughout the Chapters that follow in more complex settings - including the notions of *optimality*, *local optimization*, *descent directions*, *steplengths*, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 Visualizing minima and maxima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a function takes in only one or two inputs we can attempt to visually identify its minima or maxima by plotting it over a large swath of its input space. While this idea certainly fails when a function takes in three or more inputs (since we can no longer visualize it properly), we begin nevertheless by first examining a number of low dimensional examples to gain an intuitive feel for how we might effectively identify these desired minima or maxima in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#a50e3e;\">Example 1. </span> Visual inspection of simple functions for minima and maxima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every machine learning problem has parameters that must be tuned properly to ensure optimal learning. For example, there are two parameters that must be properly tuned in the case of a simple linear regression, when fitting a line to a scatter of data: the slope and intercept of the linear model.\n",
    "\n",
    "These two parameters are tuned by forming what is called a cost function or loss function. This is a continuous function in both parameters that measures how well the linear model fits a dataset given a value for its slope and intercept. The proper tuning of these parameters via the cost function corresponds geometrically to finding the values for the parameters that make the cost function as small as possible or, in other words, minimize the cost function. The image below illustrates how choosing a set of parameters higher on the cost function results in a corresponding line fit that is poorer than the one corresponding to parameters at the lowest point on the cost surface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"chapter_2_images/bigpicture_regression_optimization.png\" width=\"75%\" height=\"auto\" alt=\"\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPython.display.HTML('''<img src=\"''' + image_path_1 + '''\" width=\"75%\" height=\"auto\" alt=\"\"/>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This same idea holds true for regression with higher dimensional input, as well as classification where we must properly tune parameters to *separate* classes of data.\n",
    "Again, the parameters minimizing an associated cost function provide the best classification result. This is illustrated for classification below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"chapter_2_images/bigpicture_classification_optimization.png\" width=\"75%\" height=\"auto\" alt=\"\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPython.display.HTML('''<img src=\"''' + image_path_2 + '''\" width=\"75%\" height=\"auto\" alt=\"\"/>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuning of these parameters require the *minimization of a cost function* can be formally written as follows.  For a generic function $g(\\mathbf{w})$ taking in a general $N$ dimensional input $\\mathbf{w}$ the problem of finding the particular point $\\mathbf{v}$ where $g$ attains its smallest value is written formally as\n",
    "\n",
    "\\begin{equation}\n",
    "\\underset{\\mathbf{w}}{\\mbox{minimize}}\\,\\,\\,\\,g\\left(\\mathbf{w}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "As detailed in our series on the *vital elements of calculus*, the first order optimality condition characterizes solutions to this problem.  However because these conditions can very rarely be solved 'by hand' we must rely on algorithmic techniques for finding function minima (or at the very least finding points close to them).  In this part of the text we examine many algorithmic methods of *mathematical optimization*, which aim to do just this.\n",
    "\n",
    "> The tools of mathematical optimization are designed to minimize cost functions.  When applied to learning problems this corresponds to properly tuning the parameters of a learning model.  Mathematical optimization is the workhorse of machine learning / deep learning, playing a role in virtually every learning problem. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "103px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
