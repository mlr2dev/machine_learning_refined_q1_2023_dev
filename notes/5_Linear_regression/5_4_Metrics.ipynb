{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5: Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4  Regression Quality Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this Section we describe simple metrics for judging the quality of a trained regression model, as well as how to make predictions using one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append path for local library, data, and image import\n",
    "import sys\n",
    "sys.path.append('./chapter_5_images') \n",
    "\n",
    "# image paths\n",
    "image_path_1 = 'chapter_5_images/Fig_3_5_new.png'\n",
    "\n",
    "# standard imports\n",
    "import IPython\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions using a trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we denote the optimal set of weights found by minimizing a regression cost function by $\\mathbf{w}^{\\star}$ then note we can write our fully tuned linear model as \n",
    "\n",
    "\\begin{equation}\n",
    "\\text{model}\\left(\\mathbf{x},\\mathbf{w}^{\\star}\\right) =  \\mathring{\\mathbf{x}}_{\\,}^T\\mathbf{w}^{\\star}   = w_0^{\\star} + x_{1}^{\\,}w_1^{\\star} + x_{2}^{\\,}w_2^{\\star} + \\cdots + x_{N}^{\\,}w_N^{\\star}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of how we determine optimal parameters $\\mathbf{w}^{\\star}$, by minimizing a regression cost like the Least Squares or Least Absolute Deviations, we make predictions employing our linear model in the same way.  That is, given an input (whether one from our training dataset or a new input) $\\mathbf{x}_{\\,}$ we predict its output $y_{\\,}$ by passing it along with our trained weigths into our model as \n",
    "\n",
    "\\begin{equation}\n",
    "\\text{model}\\left(\\mathbf{x}_{\\,},\\mathbf{w}^{\\star}\\right)  = y_{\\,}\n",
    "\\end{equation}\n",
    "\n",
    "This is illustrated pictorially on a prototypical linear regression dataset for the case when $N=1$ in the Figure below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"chapter_5_images/Fig_3_5_new.png\" width=\"90%\" height=\"auto\" alt=\"\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPython.display.HTML('''<img src=\"''' + image_path_1 + '''\" width=\"90%\" height=\"auto\" alt=\"\"/>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<figcaption>   \n",
    "<strong>Figure 4:</strong> <em>  Once a line/hyperplane\n",
    "has been fit to a dataset via minimizing an appropriate cost function\n",
    "it may be used to predict the output value of any input. Here a\n",
    "line has been fit to a two dimensional dataset in this manner, giving\n",
    "optimal parameters $w_0^{\\star}$ and $w_1^{\\star}$, and the output value\n",
    "of a new point $x^{\\prime}$ is made using the learned linear\n",
    "model as $w_0^{\\star}+x^{\\prime}w_1^{\\star} = y^{\\prime}$. </em>  </figcaption> \n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Judging the quality of a trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have successfully minimized the a cost function for linear regression it is an easy matter to determine the quality of our regression model: we simply evaluate a cost function using our optimal weights.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then evalaute the quality of this trained model using a Least Squares cost - which is especially natural to use when we employ this cost in training.  To do this we plug in our trained model and dataset into the Least Squares cost - giving the *Mean Squared Error* (or *MSE* for short) of our trained model\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{MSE}=\\frac{1}{P}\\sum_{p=1}^{P}\\left(\\text{model}\\left(\\mathbf{x}_p,\\mathbf{w}^{\\star}\\right) -y_{p}^{\\,}\\right)^{2}.\n",
    "\\end{equation}\n",
    "\n",
    "The name for this quality measurement describes precisely what the Least Squares cost computes - the average (or *mean*) squared error.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way we can employ the Least Absolute Deviations cost to determine the quality of our trained model.  Plugging in our trained model and dataset into this cost computes the Mean Absolute Deviations (or *MAD* for short) which is precisely what this cost function computes\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{MAD}=\\frac{1}{P}\\sum_{p=1}^{P}\\left\\vert\\text{model}\\left(\\mathbf{x}_p,\\mathbf{w}^{\\star}\\right) -y_{p}^{\\,}\\right\\vert.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two measurements differ in precisely the ways we have seen their respective cost functions differ - e.g., the MSE measure is far more sensitive to *outliers*.  Which measure one employs in practice can therefore depend on personal and/or occupational preference, or the nature of a problem at hand.\n",
    "\n",
    "Of course in general the *lower* one can make a quality measures, by proper tuning of model weights, the *better* the quality of the corresponding trained model (and vice versa).  However the threshold for what one considers 'good' or 'great'  performance can depend on personal preference, an occupational or institutionally set benchmark, or some other problem-dependent concern."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "49px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
